---
title: "Word Analysis"
author: "Miha Rupar"
output: 
  github_document:
    toc: true
    toc_depth: 3
---
```{r, include=FALSE,message=FALSE,warning=FALSE}
library(htm2txt)
library(tidyverse)
library(quanteda)
library(quanteda.textplots)
library(pvclust)
```


## Data import
```{r}
url <- 'http://bos.zrc-sazu.si/sbsj.html'
text <- gettxt(url)
```


## Data procesing
Transforming data to get desired outcome for later use
```{r}
words <- text %>% str_split("\n") %>% 
  as.data.frame() %>% .[-c(1:5), ] %>%
  as.data.frame() %>% slice(., 1:(n() - 5)) 

colnames(words) <- c("besede")

words <- words %>% mutate(
  besede = besede %>%
    str_replace(" \\s*\\([^\\)]+\\)", "") %>%
    str_replace_all("\xc2\xa0", " ")%>%
    str_replace_all("-"," ") %>%
    str_trim("right") %>%
    str_replace_all(" ",".") %>%
    paste0(".")
)
```

## Calculate weight
Defining weight matrix
```{r}
letter <- c("a", "b", "c", "č", "d", "e", "f", "g", "h", "i", "j", "k", "l", 
            "m", "n", "o", "p", "r", "s", "š", "t", "u", "v", "z", "ž",".")
n_letter <- toupper(letter)

weight_mat <- matrix(0, nrow = length(n_letter), ncol = length(letter))
colnames(weight_mat) <- letter
rownames(weight_mat) <- toupper(letter)

```

Function for adding weights to matrix, based on next character in string.
Ex. "abc" => [a,b] +1 & [b,c] +1
```{r}
add_weights <- function(word,lvl){
  if(nchar(word)<lvl+1){return()}
  l1 = substring(toupper(word), 1, lvl)
  l2 = substring(tolower(word), lvl+1, lvl+1)
  if(l1 %in% toupper(n_letter) & l2 %in% tolower(letter)){
    weight_mat[l1, l2] <<- weight_mat[l1, l2] + 1
  }
  add_weights(substring(word, 2),lvl)
}

```


```{r}
filtered_words <- function(word, lvl) {
  l1 <- substring(toupper(word), 1, lvl)
  if(nchar(word) < lvl + 1) {
    return(l1 %in% toupper(n_letter))
  }
  l2 <- substring(tolower(word), lvl + 1, lvl + 1)
  if(l1 %in% toupper(n_letter) && l2 %in% tolower(letter)) {
    return(filtered_words(substring(word, 2), lvl))
  }else {
    return(FALSE)
  }
}

```

```{r}
reset <- function(cutoff){
  filtered_mat <- weight_mat
  filtered_mat[filtered_mat < cutoff] <- 0
  n_letter <<- c()
  for (i in rownames(filtered_mat)){
    for (j in colnames(filtered_mat)){
      if (filtered_mat[i,j] != 0){
        n_letter <<- c(n_letter,paste0(i,j))
      }
    }
  }
  weight_mat <<- matrix(0, ncol = 26, nrow = length(n_letter))
  colnames(weight_mat) <<- letter
  rownames(weight_mat) <<- toupper(n_letter)
}
```

```{r}
calculate_weights <- function(lvl,cutoff=1500){
  for (clvl in 1:lvl){
    if(lvl != 1){reset(cutoff)}
    for (w in 1:nrow(words)){
    if(filtered_words(words$besede[w],lvl)){
      add_weights(words$besede[w],lvl)
      }
    }
  }
}
```

```{r}
normalize <- function(mat){
  letter_freq <- mat %>% 
    rowSums() %>% as.data.frame()
  sweep(mat, 1, letter_freq$., FUN = "/") %>% return()
}
```


Calculate weights and visualize
```{r}
calculate_weights(1)
heatmap(weight_mat, Colv=NA, Rowv=NA)
```


### Clustering
```{r}
heatmap(weight_mat)

weight_mat %>% normalize() %>%
  dist(method = "canberra") %>%
  hclust(method = "average") %>%
  plot(xlab = "Letters", ylab = "Distance")

weight_mat %>% normalize() %>%
  pvclust() %>% plot()
```

### letter co-occurence
```{r}
weight_mat %>% as.dfm() %>%
  fcm() %>% textplot_network()
```




